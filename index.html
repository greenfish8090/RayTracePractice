<!doctype html>

<html lang="en">
<head>
  <meta charset="utf-8">

  <title>CG Project - RayTracing</title>
  <meta name="description" content="Process and Results">
  <meta name="author" content="PSK">

  <link rel="stylesheet" href="styles.css">

</head>

<body>
  <div class="segment">
      <div class="title">
        Introduction
      </div>
      <div class="content">
        Raytracing is an advanced and computing-intensive method of generating graphics. Unlike traditional methods of generating objects in 3D space, this method can yield much more accurate and realistic results with
        the caviat of extremely high processing requirement. While the type of raytracing we have made is only suitable for still objects due to rendering times, some versions of ray tracing or pseudo raytracing exist that
        can be used for realtime simulations.
      </div>
      </div> 
      <div class="segment">
      <div class="title">
        Initial Construction
      </div>
      <div class="content">
        We can start from the basics. Raytracing, in short, is the method of casting rays at each individual pixel on a 'screen' in front of a scene of objects in the hopes of it hitting an object. If it does, it retrieves
        the 'data' of the point of the object it hit and using that prints out a specific color on the screen. This is done for every single pixel present on screen.
        The result is therefore extremely accurate but incredibly time-wasting.
        <div class="image">
          <img src="images/theory1.png">
          <div class="label">
            Diagrammatic representation of raytracing
          </div>
        </div>
        This is a simple enough strategy to start off with. The complexity comes in the data required for the color of each pixel. The first issue is to detect whether a ray is actually hitting an object. This problem is solved
        using standard matrix geometrical calculations based on the normal of a triangle, which most objects are made out of. Of course, customized algorithms for different types of solids can be made similarly.
        <div class="image">
          <img src="images/theory2.jfif">
          <div class="label">
            Drawing a basic sphere with only hit detection; Background for display purposes
          </div>
        </div>
        Now this process becomes extremely intensive very quickly and will take too long per frame to render, so a way out of this predicament is to split the work into multiple threads. A compute shader can accomplish This
        but dividing up and workspace into multiple work groups based on your graphics card and work on each one in seperate threads. This speeds up the process exponentially and eases up the work done by the CPU in calculations.
        A compute shader is fairly simple to set up, and for the purposes of OpenGL, we make a quad texture that covers the entire window onto which a texture is rendered and the compute shader draws pixels onto the texture 
        mentioned.<br><br>
        Now this looks pretty ugly without a background, so we added a skybox. It is a bounding box around a scene that appears to the viewer as the sky and background. Essentially, it is an arbitrarily large cube containing a warped texture or image to
        simulate atmosphere. We found this one off the internet:
        <div class="image">
          <img src="images/skybox.png">
          <div class="label">
            Skybox
          </div>
        </div>
        Another task we can undertake in light bouncing. We can set up 'mirror' objects that almost purely reflect light and iteratively create origin and destination rays from one surface to another to mimic reflection. The pixel color intensity is continuously
        modified by each bounce by a certain value until a 'bounce limit' is reached. We can reach some impressive results with this alone. You can now see behind the camera as well.
        We gave the spheres a slight tint:
        <div class="image">
          <img src="images/mirror1.png">
          <div class="label">
            Reflection limit = 1
          </div>
        </div>
        This image was generated only by reflecting each camera ray once and finding the object at that location.
        That's why you see the spheres inside the reflections in solid color. Increase that limit, and you can see reflections inside reflections, and so on:
        <div class="image">
          <img src="images/mirror2.png">
          <div class="label">
            Reflection limit = 2
          </div>
        </div>
        <div class="image">
          <img src="images/mirror3.png">
          <div class="label">
            Reflection limit = 3
          </div>
        </div>
        <div class="image">
          <img src="images/mirror5pure.png">
          <div class="label">
            Purely reflective spheres with limit = 5
          </div>
        </div>
        <div class="image">
          <img src="images/mirror5strong.png">
          <div class="label">
            Strongly tinted spheres with limit = 5
          </div>
        </div>
      </div>
    </div>
  </div>
    <div class="segment">
      <div class="title">
        Photorealistic Rendering
      </div>
      <div class="content">
        Now, we can process some basic lighting, which can be as simple as checking for light bouncing off an object. If we were to process every light ray emitted from a light source and check for objects it hit it would be
        too much even for a compute shader to process, so a workaround is to process light in the opposite direction, i.e from the camera eye to the light source. By checking the angle between a light ray and the ray we
        use to trace objects we can achieve some sort of primitive lighting.
        <div class="image">
          <img src="images/primitivelight.jfif">
          <div class="label">
            Primitive lighting model
          </div>
        </div>
        Now we are prepared to start applying James T Kajiya's rendering equation to get better results for not just mirrors but all kinds of surfaces:
        <div class="image">
          <img src="images/equation1.png">
          <div class="label">
            Rendering Equation
          </div>
        </div>
        The left hand side of the equation denotes the light coming out of a point x, and is eventually what we want. The first term on RHS is the light emitted. This would be 0 for
        most objects, non zero for light sources. The integral describes the light that the point accumulates from the surroundings. <br>
        The integral is applied over the hemisphere around the point in question, in every direction.
        The first function right after the integral is the bidirectional reflectance distribution function, BRDF. This function takes as input an incoming direction and an outgoing
        direction and spits out a value that correlates the two. For example, for a perfect mirror, the BRDF will produce 0 for every (incoming, outgoing) pair except the ones
        that have their angle of reflection equal to angle of incidence. Other surfaces will have a more complicated correlation.
        <div class="image">
          <img src="images/brdf.png">
          <div class="label">
            Crude visualization of what the BRDF does
          </div>
        </div>
        Note that "incoming" in BRDF terminology is represented as outgoing rays in the diagram because this is ray tracing, i.e., tracing the ray backwards. What actually happens
        in nature is that all those "outgoing" rays in the diagram come to the surface, merge into the "incoming" ray (the dark one) and hit our eye.
        <br><br>
        The term that comes right after it is the Lambert term which represents the Lambert cosine law. 
        <div class="image">
          <img src="images/lambert.webp">
          <div class="label">
            Lambert term explanation
          </div>
        </div>
        Finally, the last term is, well, the light coming from a given incoming direction. However will we calculate this? Simple, just apply the same rendering equation at that
        point... and the point from there, and so on.
        You might start to see why this would be a problem. This is infinitely recursive and there is no way to expect a simulation to actually run this. So we have to make
        some compromises. One, we have to pick only a few rays to shoot out, not an infinite number of them. Second, we need a max reflections limit to prevent a ray from reflecting
        infinitely. Here is a very naive approach to realizing this solution:
        <div class="big">
          <img src="images/naiveraytracer.png">
          <div class="label">
          </div>
        </div>
        It uses recursion and a ray stops reflecting when it encounters a light source.
        There are two issues with this: One, GLSL (The OpenGL Shading Language) doesn't support recursion (We can still manage to do it by using stacks but it's going to be messy
        and too mentally taxing) and two, it's going to take a very long time to render even if it did.
        We have to find a better solution.
        <br>
        Fortunately, people have already faced this issue before and have come up with elegant solutions
        <br><br>
        We can apply Monte Carlo Integration to the integral
        <div class="image">
          <img src="images/monte.png">
          <div class="label">
          </div>
        </div>
        The integral of a function can now be calculated by averaging random samples in the specified way, where p(xn) denotes the probability of the sample being chosen. Since we
        are sampling over a hemisphere, this value will be 1/2Ï€. Our rendering equation is now:
        <div class="image">
          <img src="images/equation2.png">
          <div class="label">
          </div>
        </div>
        It now reduces to a simple iterative fraction addition. This can be easily achieved using Blending methods in OpenGl and varying the alpha
        (transparency) value of each pixel after each frame of iteration. Then, instead of exactly calculating every single pixel value of each farme exactly, we can instead use
        randomized distribution as our main crutch. One single frame using randomized values would look horrible, but an accumulated iteration of slowly dimishing samples
        will drag the image into an eventually photorealistic image because we are increasing the N frame by frame. It will again be quite time intensive to reach enough frames to
        see good results, but much less than the theoretical requirement.
        <br>
        Next, we have to define a BRDF.
        <div class="image">
          <img src="images/newBrdf.png">
          <div class="label">
          </div>
        </div>
        The first term represents diffuse reflection and the second, specular reflection. kd is the albedo of the surface and ks is the specular. (They control the strengths of
        these types of reflections). In the case of specular reflection, we also have wr and alpha. wr is just the perfectly reflected ray, and alpha works like this:
        <div class="image">
          <img src="images/alpha.gif">
          <div class="label">
          </div>
        </div>
        As you can see, it controls the spread of the specular (mirror-like) reflection
      </div>
  </div>
  <div class = "segment">
    <div class = "title">
      Piecing things together
    </div>
    <div class = "content">
      Now that we have our code, it's time to see what it can do.
      <div class="image">
        <img src="images/example1frame1.png">
        <div class="label">
          First frame
        </div>
      </div>
      Well, the first frame looks horrible, as expected. If we let it run for a while and let the Monte Carlo summation do its magic, we get:
      <div class="image">
        <img src="images/example1framen.png">
        <div class="label">
          Frame after around 50 cycles
        </div>
      </div>
      Sick. Let's break down the scene:
      <div class="image">
        <img src="images/example1explanation.png">
        <div class="label">
        </div>
      </div>
      If we make the ground a perfect mirror, (set alpha very high):
      <div class="image">
        <img src="images/example2.png">
        <div class="label">
          Very reflective ground
        </div>
      </div>
      If we make the ground non reflective:
      <div class="image">
        <img src="images/example3.png">
        <div class="label">
          Diffuse ground
        </div>
      </div>
      Right now, the skybox is acting as a light source with a non-zero emission value. If we make that 0, we get to see the smaller light sources in action:
      <div class="image">
        <img src="images/example4.png">
        <div class="label">
          Dark skybox
        </div>
      </div>
      We can even make the ground emissive:
      <div class="image">
        <img src="images/example5.png">
        <div class="label">
          Weakly emissive ground emitting an RGB of (0.2, 0.1, 0.1)
        </div>
      </div>
    </div>
  </div>
</body>
</html>